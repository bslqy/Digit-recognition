{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Digit Recognition\n",
    "\n",
    "## Statistical Machine Learning (COMP90051), Semester 2, 2017\n",
    "\n",
    "*Copyright the University of Melbourne, 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted by:  *your name here*\n",
    "### Student number: *your number here*\n",
    "### Kaggle-in-class username: *your username here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will be applying machine learning for recognising digits from real world images. The project worksheet is a combination of text, pre-implemented code and placeholders where we expect you to add your code and answers. You code should produce desired result within a reasonable amount of time. Please follow the instructions carefully, **write your code and give answers only where specifically asked**. In addition to worksheet completion, you are also expected to participate **live competition with other students in the class**. The competition will be run using an on-line platform called Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Marking:** You can get up to 33 marks for Project 2. The sum of marks for Project 1 and Project 2 is then capped to 50 marks\n",
    "\n",
    "**Due date:** Wednesday 11/Oct/17, 11:59pm AEST (LMS components); and Kaggle competition closes Monday 09/Oct/17, 11:59pm AEST.\n",
    "\n",
    "**Late submissions** will incur a 10% penalty per calendar day\n",
    "\n",
    "** Submission materials**\n",
    " - **Worksheet**: Fill in your code and answers within this IPython Notebook worksheet.\n",
    " - **Competition**: Follow the instructions provided in the corresponding section of this worksheet. Your competition submissions should be made via Kaggle website.\n",
    " - **Report**: The report about your competition entry should be submitted to the LMS as a PDF file (see format requirements in `2.2`).\n",
    " - **Code**: The source code behind your competition entry.\n",
    "The **Worksheet**, **Report** and **Code** should be bundled into a `.zip` file (not 7z, rar, tar, etc) and submitted in the LMS. Marks will be deducted for submitting files in other formats, or we may elect not to mark them at all.\n",
    "\n",
    "**Academic Misconduct:** Your submission should contain only your own work and ideas. Where asked to write code, you cannot re-use someone else's code, and should write your own implementation. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "1. Handwritten Digit Recognition **(16 marks)**\n",
    "  1. Linear Approach\n",
    "  2. Basis Expansion\n",
    "  3. Kernel Perceptron\n",
    "  4. Dimensionality Reduction\n",
    "  \n",
    "2. Kaggle Competition **(17 marks)**\n",
    "  1. Making Submissions\n",
    "  2. Method Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handwritten Digit Recognition\n",
    "Handwritten digit recognition can be framed as a classification task: given a bitmap image as input, predict the digit type (0, 1, ..., 9). The pixel values in each position of the image form our features, and the digit type is the class. We are going to use a dataset where the digits are represented as *28 x 28* bitmap images. Each pixel value ranges between 0 and 1, and represents the monochrome ink intensity at that position. Each image matrix has been flattened into one long feature vector, by concatenating each row of pixels.\n",
    "\n",
    "In this part of the project, we will only use images of two digits, namely \"7\" and \"9\". As such, we will be working on a binary classification problem. *Throughout this first section, our solution is going to be based on the perceptron classifier.*\n",
    "\n",
    "Start by setting up working environment, and loading the dataset. *Do not override variable `digits`, as this will be used throughout this section.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "digits = np.loadtxt('digits_7_vs_9.csv', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time to explore the dataset. Note that each image of \"7\" is labeled as -1, and each image of \"9\" is labeled as +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a stack of 28x28 bitmaps\n",
    "X = digits[:, 0:784]\n",
    "\n",
    "# extract labels for each bitmap\n",
    "y = digits[:, 784:785]\n",
    "\n",
    "# display a single bitmap and print its label\n",
    "bitmap_index = 0\n",
    "plt.imshow(X[bitmap_index,:].reshape(28, 28), interpolation=None)\n",
    "print(y[bitmap_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also display several bitmaps at once using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gallery(array, ncols):\n",
    "    nindex, height, width = array.shape\n",
    "    nrows = nindex//ncols\n",
    "    result = (array.reshape((nrows, ncols, height, width))\n",
    "              .swapaxes(1,2)\n",
    "              .reshape((height*nrows, width*ncols)))\n",
    "    return result\n",
    "\n",
    "ncols = 10\n",
    "result = gallery(X.reshape((300, 28, 28))[:ncols**2], ncols)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(result, interpolation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Linear Approach\n",
    "We are going to use perceptron for our binary classification task. Recall that perceptron is a linear method. Also, for this first step, we will not apply non-linear transformations to the data.\n",
    "\n",
    "Implement and fit a perceptron to the data above. You may use the implementation from *sklearn*, or implementation from one of our workshops. Report the error of the fit as the proportion of misclassified examples.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of a linear approach is the ability to interpret results. To this end, plot the parameters learned above. Exclude the bias term if you were using it, set $w$ to be the learned perceptron weights, and run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(w.reshape(28,28), interpolation=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences, describe what you see, referencing which features are most important for making classification. Report any evidence of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and heldout validation partitions by holding out a random 25% sample of the data. Evaluate the error over the course of a training run, and plot the training and validation error rates as a function of the number of passes over the training dataset.\n",
    "\n",
    "<br />\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences, describe the shape of the curves, and compare the two. Now consider if we were to stop training early, can you choose a point such that you get the best classification performance? Justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tried a simple approach, we are going to implement several non-linear approaches to our task. Note that we are still going to use a linear method (the perceptron), but combine this with a non-linear data transformation. We start with basis expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basis Expansion\n",
    "Apply Radial Basis Function (RBF)-based transformation to the data, and fit a perceptron model. Recall that the RBF basis is defined as\n",
    "\n",
    "$$\\varphi_l(\\mathbf{x}) =  \\exp\\left(-\\frac{||\\mathbf{x} - \\mathbf{z}_l||^2}{\\sigma^2}\\right)$$\n",
    "\n",
    "where $\\mathbf{z}_l$ is centre of the $l^{th}$ RBF. We'll use $L$ RBFs, such that $\\varphi(\\mathbf{x})$ is a vector with $L$ elements. The spread parameter $\\sigma$ will be the same for each RBF.\n",
    "\n",
    "*Hint: You will need to choose the values for $\\mathbf{z}_l$ and $\\sigma$. If the input data were 1D, the centres $\\mathbf{z}_l$ could be uniformly spaced on a line. However, here we have 784-dimensional input. For this reason you might want to use some of the training points as centres, e.g., $L$ randomly chosen \"2\"s and \"7\"s.*\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the validation error for your RBF-perceptron and use this to choose good values of $L$ and $\\sigma$. Show a plot of the effect of changing each of these parameters, and justify your parameter choice.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your justfication here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Kernel Perceptron\n",
    "Next, instead of directly computing a feature space transformation, we are going to use the kernel trick. Specifically, we are going to use the kernelised version of perceptron in combination with a few different kernels.\n",
    "\n",
    "*In this section, you cannot use any libraries other than `numpy` and `matplotlib`.*\n",
    "\n",
    "First, implement linear, polynomial and RBF kernels. The linear kernel is simply a dot product of its inputs, i.e., there is no feature space transformation. Polynomial and RBF kernels should be implemented as defined in the lecture slides.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# u,v - column vectors of the same dimensionality\n",
    "#\n",
    "# Output:\n",
    "# v - a scalar\n",
    "def linear_kernel(u, v):\n",
    "    ## your code here\n",
    "\n",
    "# Input:\n",
    "# u,v - column vectors of the same dimensionality\n",
    "# c,d - scalar parameters of the kernel as defined in lecture slides\n",
    "#\n",
    "# Output:\n",
    "# v - a scalar\n",
    "def polynomial_kernel(u, v, c=0, d=3):\n",
    "    ## your code here\n",
    "\n",
    "# Input:\n",
    "# u,v - column vectors of the same dimensionality\n",
    "# gamma - scalar parameter of the kernel as defined in lecture slides\n",
    "#\n",
    "# Output:\n",
    "# v - a scalar\n",
    "def rbf_kernel(u, v, gamma=1):\n",
    "    ## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel perceptron was a \"green slides\" topic, and you will not be asked about this method in the exam. Here, you are only asked to implement a simple prediction function following the provided equation. In kernel perceptron, the prediction for instance $\\mathbf{x}$ is made based on the sign of\n",
    "\n",
    "$$w_0 + \\sum_{i=1}^{n}\\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x})$$\n",
    "\n",
    "Here $w_0$ is the bias term, $n$ is the number of training examples, $\\alpha_i$ are learned weights, $\\mathbf{x}_i$ and $y_i$ is the training dataset,and $K$ is the kernel.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# x_test - (r x m) matrix with instances for which to predict labels\n",
    "# X - (n x m) matrix with training instances in rows\n",
    "# y - (n x 1) vector with labels\n",
    "# alpha - (n x 1) vector with learned weigths\n",
    "# bias - scalar bias term\n",
    "# kernel - a kernel function that follows the same prototype as each of the three kernels defined above\n",
    "#\n",
    "# Output:\n",
    "# y_pred - (r x 1) vector of predicted labels\n",
    "def kernel_ptron_predict(x_test, X, y, alpha, bias, kernel):\n",
    "    ## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for kernel perceptron training is provided below. You can treat this function as a black box, but we encourage you to understand the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# X - (n x m) matrix with training instances in rows\n",
    "# y - (n x 1) vector with labels\n",
    "# kernel - a kernel function that follows the same prototype as each of the three kernels defined above\n",
    "# epochs - scalar, number of epochs\n",
    "#\n",
    "# Output:\n",
    "# alpha - (n x 1) vector with learned weigths\n",
    "# bias - scalar bias term\n",
    "def kernel_ptron_train(X, y, kernel, epochs=100):\n",
    "    n, m = X.shape\n",
    "    alpha = np.zeros(n)\n",
    "    bias = 0\n",
    "    updates = None\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch =', epoch, ', updates =', updates)\n",
    "        updates = 0\n",
    "\n",
    "        schedule = list(range(n))\n",
    "        np.random.shuffle(schedule)\n",
    "        for i in schedule:\n",
    "            y_pred = kernel_ptron_predict(X[i], X, y, alpha, bias, kernel)\n",
    "            \n",
    "            if y_pred != y[i]:\n",
    "                alpha[i] += 1\n",
    "                bias += y[i]\n",
    "                updates += 1\n",
    "\n",
    "        if updates == 0:\n",
    "            break\n",
    "        \n",
    "    return alpha, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the above functions to train the perceptron. Use heldout validation, and compute the validation error for this method using each of the three kernels. Write a paragraph or two with analysis how the accuracy differs between the different kernels and choice of kernel parameters. Discuss the merits of a kernel approach versus direct basis expansion approach as was used in the previous section.\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color='red'>**Provide your analysis here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dimensionality Reduction\n",
    "Yet another approach to working with complex data is to use a non-linear dimensionality reduction. To see how this might work, first apply a couple of dimensionality reduction methods and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "X = digits[:, 0:784]\n",
    "y = np.squeeze(digits[:, 784:785])\n",
    "\n",
    "# n_components refers to the number of dimensions after mapping\n",
    "# n_neighbors is used for graph construction\n",
    "X_iso = manifold.Isomap(n_neighbors=30, n_components=2).fit_transform(X)\n",
    "\n",
    "# n_components refers to the number of dimensions after mapping\n",
    "embedder = manifold.SpectralEmbedding(n_components=2, random_state=0)\n",
    "X_se = embedder.fit_transform(X)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(X_iso[y==-1,0], X_iso[y==-1,1], \"bo\")\n",
    "ax1.plot(X_iso[y==1,0], X_iso[y==1,1], \"ro\")\n",
    "ax1.set_title('Isomap')\n",
    "ax2.plot(X_se[y==-1,0], X_se[y==-1,1], \"bo\")\n",
    "ax2.plot(X_se[y==1,0], X_se[y==1,1], \"ro\")\n",
    "ax2.set_title('spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences, explain how a dimensionality reduction algorithm can be used for your binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement such an approach and assess the result. For simplicity, we will assume that both training and test data are available ahead of time, and thus the datasets should be used together for dimensionality reduction, after which you can split off a test set for measuring generalisation error. *Hint: you do not have to reduce number of dimensions to two. You are welcome to use the sklearn library for this question.*\n",
    " \n",
    "<br />\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences, comment on the merits of the dimensionality reduction based approach compared to linear classification from Section 1.1 and basis expansion from Section 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here ...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kaggle Competition\n",
    "The final part of the project is a competition, on more challenging digit data sourced from natural scenes. This data is coloured, pixelated or otherwise blurry, and the digits are not perfectly centered. It is often difficult for humans to classify! The dataset is also considerably larger. \n",
    "\n",
    "Please sign up to the [COMP90051 Kaggle competition](https://inclass.kaggle.com/c/comp90051-2017) using your `student.unimelb.edu.au` email address. Then download the file `data.npz` from Kaggle. This is a compressed `numpy` data file containing three ndarray objects:\n",
    " - `train_X` training set, with 4096 input features (greyscale pixel values);\n",
    " - `train_Y` training labels (0-9)\n",
    " - `test_X` test set, with 4096 input features, as per above\n",
    " \n",
    "Each image is 64x64 pixels in size, which has been flattened into a vector of 4096 values. You should load the files using `np.load`, from which you can extract the three elements. You may need to transpose the images for display, as they were flattened in a different order. Each pixel has an intensity value between 0-255. For those using languages other than python, you may need to output these objects in another format, e.g., as a matlab matrix.\n",
    "\n",
    "Your job is to develop a *multiclass* classifier on this dataset. You can use whatever techniques you like, such as the perceptron code from above, or other methods such as *k*NN, logistic regression, neural networks, etc. You may want to compare several methods, or try an ensemble combination of systems. You are free to use any python libraries for this question. Note that some fancy machine learning algorithms can take several hours or days to train (we impose no time limits), so please start early to allow sufficient time. *Note that you may want to sample smaller training sets, if runtime is an issue, however this will degrade your accuracy. Sub-sampling is a sensible strategy when developing your code.*\n",
    "\n",
    "You may also want to do some basic image processing, however, as this is not part of the subject, we would suggest that you focus most of your efforts on the machine learning. For inspiration, please see [Yan Lecun's MNIST page](http://yann.lecun.com/exdb/mnist/), specifically the table of results and the listed papers. Note that your dataset is harder than MNIST, so your mileage may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Making Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be setup as a *Kaggle in class* competition, in which you can upload your system predictions on the test set. You should format your predictions as a csv file, with the same number of lines as the test set, and each line comprising two numbers `id, class` where *id* is the instance number (increasing integers starting from 1) and *class* is an integer between 0-9, corresponding to your system prediction. E.g., \n",
    "```\n",
    "Id,Label\n",
    "1,9\n",
    "2,9\n",
    "3,4\n",
    "4,5\n",
    "5,1\n",
    "...```\n",
    "based on the first five predictions of the system being classes `9 9 4 5 1`. See the `sample_submission.csv` for an example file.\n",
    "\n",
    "Kaggle will report your accuracy on a public portion of the test set, and maintain a leaderboard showing the performance of you and your classmates. You will be allowed to upload up to four submissions each day. At the end of the competition, you should nominate your best submission, which will be scored on the private portion of the test set. The accuracy of your system (i.e., proportion of correctly classified examples) on the private test set will be used for grading your approach.\n",
    "\n",
    "**Marks will be assigned as follows**:\n",
    " - position in the class, where all students are ranked and then the ranks are linearly scaled to <br>0 marks (worst in class) - 4 marks (best in class) \n",
    " - absolute performance (4 marks), banded as follows (rounded to nearest integer): \n",
    " <br>below 80% = 0 marks; 80-89% = 1; 90-92% = 2; 93-94% = 3; above 95% = 4 marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you are required to submit your code with this notebook, submitted to the LMS. Failure to provide your implementation may result in assigning zero marks for the competition part, irrespective of the competition standing. Your implementation should be able to exactly reproduce submitted final Kaggle entry, and match your description below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Method Description\n",
    "Describe your approach, and justify each of the choices made within your approach. You should write a document with no more than 400 words, as a **PDF** file (not *docx* etc) with up to 2 pages of A4 (2 sides). Text must only appear on the first page, while the second page is for *figures and tables only*. Please use a font size of 11pt or higher. Please consider using `pdflatex` for the report, as it's considerably better for this purpose than wysiwyg document editors. You are encouraged to include empirical results, e.g., a table of results, graphs, or other figures to support your argument. *(this will contribute 9 marks; note that we are looking for clear presentation, sound reasoning, good evaluation and error analysis, as well as general ambition of approach.)*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
